---
title: "'rnaseq_count_nf' Nextflow Pipeline Instructions"
author: "Jenny L Smith"
date: "`r Sys.Date()`"
params:
  outdir:
    value: ""
    input: text
output:
    html_document:
        theme: readable
        toc: true
        toc_float: true
        toc_depth: 3
        number_sections: true
        fig_caption: true
        df_print: paged
    github_document:
        html_preview: false
        toc: true
        fig_width: 5
        fig_height: 5
        toc_depth: 3
editor_options: 
    markdown: 
        wrap: 72
---


```{r set-up, eval=TRUE, echo=FALSE,  message=FALSE, warning=FALSE}
# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

knitr::opts_chunk$set(
  tidy.opts = list(width.cutoff = 50),
  tidy = TRUE,
  fig.align = "center",
  fig.width = 10, fig.height = 10,
  eval = FALSE
)

options(stringsAsFactors = FALSE, max.print = 100)
table <- function(..., useNA = "ifany") base::table(..., useNA = useNA)

library(dplyr)
library(stringr)
```


# About the Workflow

This workflow is designed to output gene expression counts from STAR aligner using [`--quantmode`](https://physiology.med.cornell.edu/faculty/skrabanek/lab/angsd/lecture_notes/STARmanual.pdf). It will also perform general QC statistics on the fastqs with [fastqc](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and the alignment using [rseqc](https://rseqc.sourceforge.net/). Finally, the QC reports are collected into a single file using [multiQC](https://multiqc.info/).

A DAG (directed acyclic graph) of the workflow is show below:

```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("../images/dag.png")
```

# Activate the Environment on HPC

The directions to set-up the Nextflow workflow requirements are found in
the [README.md](../README.md). Ensure that you have followed the steps
to fork and clone the repository and created the conda nextflow
environment before starting with this document.

### 1) Interactive Session

Optional but recommended: use `tmux` on the cybertron login nodes. Name
the session nextflow and then request an interactive session, then
activate the nextflow conda environment. The project codes can be found
with `project info` command. Change the `$QUEUE` and `$NAME` variables
in the code chunk below to be accurate for your Cybertron projects.

```{bash optional, eval=FALSE}
tmux new-session -s nextflow
project info
NAME="RSC_adhoc"
QUEUE="paidq"
qsub -I -q $QUEUE -P $(project code $NAME) -l select=1:ncpus=1:mem=8g -l walltime=8:00:00
```

### 2) Open `rnaseq_count_nf` workflow folder

Navigate to where you place the cloned (copied) cutandrun_nf directory, and then checkout the latest release branch.

```{bash eval=FALSE}
cd /path/to/my/rnaseq_count_nf
git fetch

# this will list all branches of the repository. Find the release branch with the latest version in red text, which means you don't yet have a local copy of that branch. 
git branch -a

# then select the latest version, for example 2.0.0. This downloads the stable version of pipeline locally. 
git checkout release/2.0.0

# Now the * indicates that you're on the release branch and its no longer red text. 
git branch
```

### 3) Activate conda environement

Activate the Nextflow conda environment.

```{bash required, eval=FALSE}
conda env create -f env/nextflow.yaml
conda activate nextflow
```

# Test the Workflow 

## Edit the Config File 

Edit the `nextflow.config` file in any text editor; the example below is in R. 

You will need to change the:

 * project code (use the same one as you used above), 
 * the queue name to be paidq or a tier 3 queue.
 
 Paidq will cost less than $0.01 for testing with the workflow's example data provided in the directory `test_data`.

```
//global parameters
params {
    // general options
    sample_sheet                = "test_data/paired_end_sample_sheet.csv"
    download_sra_fqs            = false
    queue                       = 'paidq'
    project                     = '[PROJECT CODE]'
<...continues...>
```

```{r}
usethis::edit_file("../nextflow.config")
```


## Paired-end example

Determine if the workflow works on your installation of the conda environment by running the following command. 

```{bash}
./main_run.sh "paired_end_test"
```

## Single-end example 

To test the single-end sheet, modify the sample_sheet parameter in the `nextflow.config` and the output directory (`outdir`).

```
params {
    // general options
    sample_sheet                = "test_data/single_end_sample_sheet.csv"
    [...]
    outdir                      = "./single_end_results/"
<...continues...>
}
```

then run the command

```{bash}
./main_run.sh "single_end_test"
```


## sra download example

To test the sra sample sheet - modify these the sample sheet, and set `download_sra_fastqs` to true in the `nextflow.config` and the output directory (`outdir`).  

```
params {
    // general options
    sample_sheet                = "test_data/sra_sample_sheet.csv"
    download_sra_fqs            = true
    [...]
    outdir                      = "./sra_results/"
<...continues...>
}
```

then run on the command:

```{bash}
./main_run.sh "sra_test"
```


# Modify the Pipeline for Your Data


## Define Input Files 

A comma delimited (csv) sample sheet is required for the input samples to be processed. Please note, ***do not remove*** the comment lines that begin with "#" in the example files. The same number of comment lines *must* be included in any input sample sheet, based on the examples provided here. 

It must have the column names (in any order):

  * r1 - the filepath for the read 1 fastq in paired-end RNA-seq, or the single-end fastq file
    
  * r2 - the filepath for the read 2 fastq in paired-end RNA-seq
    
  * id - unique sample ID, no duplicates allowed in the sample sheet
    
  * single_end - boolean [true/false] if the data is single-end or paired-end 

The two examples are provided here to look at: 

```{r eval=TRUE}
example_sheet <- read.csv(here::here("test_data/paired_end_sample_sheet.csv"),
                            header = TRUE, 
                            comment.char = "#")
example_sheet
```

If downloading the fastq files directly from the SRA, the sample sheet only requires the `id` and the `single_end` columns. 

```{r eval=TRUE}
sra_example <- read.csv(here::here("test_data/sra_sample_sheet.csv"),
                        header = TRUE, 
                        comment.char = "#")

sra_example
```


## Nextflow Config

Edit the `nextflow.config` file to include the appropriate filepaths for the samples to be included in the pipeline, and the appropriate genome references. The required files are listed here:

```{r warning=FALSE, echo=FALSE, eval=TRUE}
config_file <- readLines(here::here("nextflow.config")) %>% 
    noquote() 
```

```{r echo=FALSE, eval=TRUE}
config_file %>% 
    head(., n=20) %>% 
    c(., "<...>") %>% 
    cat(.,sep = "\n")
```

## Genome References 

Genomic references including genomic fastas, and STAR indexes may be available at the following shared location on the HPC: `/gpfs/shared_data/`. Currently, there are STAR indexes for human, and  mouse. 

#### Rseqc reference

Rseqc reference bed files are generated using the provided GTF file in params section of the `nextflow.config` file. The ref_gene_model is generated in BED12 format using UCSC utilities (Kent tools). This allows the Rseqc references to match the transcript IDs used in the GTF for STAR aligner. The pipeline does require the user to provide a list of rRNA transcript IDs that match the transcript ID format in the provided GTF. 

The easiest way to find the rRNA transcripts is to use [Ensembl Biomart](http://useast.ensembl.org/biomart/martview/d3ad0d42cda078c0acc2a4b2c18a9164) or [UCSC table browser](https://genome.ucsc.edu/cgi-bin/hgTables).

For Ensembl, use a filter based on the rRNA biotypes you would like to quantify (eg Mt_rRNA , rRNA , rRNA_pseudogene) and then select "Transcript stable ID" as the Attribute to save to file. 

In R, this could be accomplished with:
```
library(biomaRt)
species <- "Homo_sapiens"
mart <- useEnsembl('ensembl', dataset = 'Homo_sapiens_gene_ensembl')
biomaRt::getBM(values=c("rRNA", "rRNA_pseudogene","Mt_rRNA"),
               filters="biotype", 
               attributes=c("ensembl_transcript_id"), 
               mart = mart)
```

An example of the gene models and rRNA bed files can be found at the RSEQC [documentation page](http://rseqc.sourceforge.net/#download-gene-models-update-on-12-14-2021) and are located at `/gpfs/shared_data/rseqc` to share with the SCRI. 

## Advanced Options

In the `nextflow.config`, you can define additional command line arguments to the scientific software under [process scope](https://www.nextflow.io/docs/latest/process.html#).  

```{r echo=FALSE, eval=TRUE}
process_lines <- c(grep("Computational resource allocation",config_file),
                   grep("Create profiles", config_file)-1)

config_file[process_lines[1]:process_lines[2]] %>% 
    head(., n=15) %>% 
    c(., "<...>") %>% 
    cat(.,sep = "\n")
```
You may use the advanced options to change computational resources requested for different processes. The CPUs and memory parameters can updated to request a larger amount of resources like CPUs or memory if files are large. You may also edit the commandline parameters for processes in the workflow using the `ext.args` [directive](https://www.nextflow.io/docs/latest/process.html#directives).

The current TRIMGALORE paramters look like this:

```{r echo=FALSE, eval=TRUE}
config_file[c(grep("Trimgalore", config_file)):c(grep("FastQC", config_file)-1)] %>% 
    c(., "<...>") %>%
    cat(.,sep = "\n")
```

But you'd like to request 16Gb of memory for and gzip the output. 

```{r echo=FALSE, eval=TRUE}
config_file[c(grep("Trimgalore", config_file)):c(grep("FastQC", config_file)-1)] %>% 
    gsub("8.GB","16.GB",.) %>% 
    gsub("= \\'\\'","= '--gzip'",.) %>% 
    c(., "<...>") %>%
    cat(.,sep = "\n")
```

# Run the workflow 

Then execute a wrapper around the `nextflow run main.nf` command which is in the `main_run.sh` shell script. Provide a descriptive name (string) for your workflow run, in this example we will use "my_analysis". 

```{r eval=TRUE, echo=FALSE}
main_run <- readLines(here::here("main_run.sh"))
```

Typically, you will not need to change the `main_run.sh` often. 

The `main_run.sh` script defines the profiles for different executors in the variable `NFX_PROFILE`. The choices for profiles are: 

* PBS_singularity [default]
* local_singularity

"PBS_singularity" is recommended. This profiles executes the jobs on the HPC using the PBS scheduler and then will run the job inside singularity containers with the appropriate scientific software versions. 

"local_singularity" is good for workflow development if you're making a lot of changes. This will use singularity on Cybertron, but run the jobs on the interactive compute node that you've requested during "Set-up  Nextflow Environment" steps above. 

```{r eval=TRUE, echo=FALSE}
main_run %>% 
    head(.,  n=10) %>% 
    c(., "<...>") %>%
    cat(.,sep = "\n")
```

 You can also change the [`entry_point`](https://www.nextflow.io/docs/latest/dsl2.html#workflow-entrypoint) for the workflow.
 
 * Run only the index building step using `NFX_ENTRY='star_index'`. 
 * Run only the download step for SRA files by setting `NFX_ENTRY='sra_fastqs'`. 
 * Keep the default `NFX_ENTRY='rnaseq_count'` to run the complete pipeline. 

```{bash}
./main_run.sh "my_analysis"
```


# Expected Outputs 

Under the path provided in the nextflow config for params "outdir", lets say `paired_end_results/`, you
will find directories named for each of the modules.

In addition, there will be an HTML report with information on where the temp data is stored in the `workDir` path, and general run statistics such as resource utilized  versus requested, which helps with optimization. It will also provide information on how much walltime was used per sample, total CPU hours, etc. 

The HTML file is found in `reports` directory and will have the prefix defined on the command line when the `./main_run.sh "my_analysis"` was invoked, so in this example it would be named "my_analysis_{DATE}.html". 

```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("../images/html_report.png")
```

There will also be a detailed nextflow log file that is useful for de-bugging which will also be named in this example, "my_analysis_{DATE}_nextflow.log".

Finally, the pipeline will produce a DAG - Directed acyclic graph which describes the workflow channels (inputs) and the modules. The DAG image will be saved under `dag/` directory with the name "my_analysis_{DATE}_dag.pdf". 

```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("../images/dag.png")
```

### Complete File Structure

There will be the following file structure:

```{r eval=TRUE, echo = FALSE}
outdir <- params$outdir

if ( outdir == "" ) {
    outdir <- config_file[grep("\\soutdir.+\ = (.+)$", config_file)] %>% 
        str_split_fixed(., pattern = ' = ', n = 2) %>% 
        .[,2] %>% 
        gsub('\\"', "", . ) %>% 
        gsub("\\.", "..", .)
}

fs::dir_tree(outdir, recurse = 1)
```

### Detailed File Structure

Within each directory you will find the following files (top 5 files per
directory are shown):

```{r eval=TRUE, echo = FALSE}
results_dirs <- fs::dir_info(outdir, recurse = TRUE)

results_dirs %>% 
    select(path, type) %>% 
    mutate(process = gsub(outdir,"", path), 
           filename = ifelse(type == 'file', basename(path), "")) %>%
    filter(!grepl("multiqc_report|params[2-9]", process)) %>% 
    mutate_at(vars(process), ~ case_when(
                   type == 'file' & grepl("fastqc", .) ~ dirname(dirname(.)),
                   grepl("FASTQC", .) ~ dirname(.),
                   type == 'file' ~ dirname(.),
                   TRUE ~ .)) %>%
    group_by(process) %>%
    dplyr::slice(1:5) %>% 
    ungroup() %>% 
    knitr::kable()
```

# Share the Data

```{bash}
RESULTS="PATH/TO/PIPELINE/RESULTS/"
OUTDIR="path/to/collabs/RSS"
rsync -av $RESULTS $OUTDIR 
```


# Cleaning up Cached Data

Nextflow has an utility to [clean](https://www.nextflow.io/docs/latest/cli.html#clean) up old work directories and logs that are no longer needed. This can be run after any amount of time to keep your workdir from getting too large or if you're running out of disk space. 

This requires the session ID or session name, which can found in the `.nextflow/history` file. 

```{bash}
nextflow log
nextflow clean -f [RUN NAME]
```



# Session Information

```{r}
sessionInfo()
```


