---
title: "Nextflow 'rnaseq_count_nf' Pipeline Run Instructions"
author: "Jenny L Smith"
date: "`r Sys.Date()`"
always_allow_html: true
output:
  html_document:
    theme: yeti
    highlight: breezedark
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
    df_print: paged
---

# Set-up 

```{r set-up, eval=TRUE, echo=TRUE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```


```{r eval=TRUE}
knitr::opts_chunk$set(
  tidy.opts = list(width.cutoff = 50),
  tidy = TRUE,
  fig.align = "center",
  fig.width = 10, fig.height = 10,
  eval = FALSE
)

options(stringsAsFactors = FALSE, max.print = 100)
table <- function(..., useNA = "ifany") base::table(..., useNA = useNA)
```

```{r}
library(dplyr)
```

# Set-up  Nextflow Environment 

```{bash}
git clone https://childrens-atlassian/bitbucket/projects/RPDEV/repos/rnaseq_count_nf/
conda env create -f env/nextflow.yaml
conda activate nextflow
```

# Define Input Files 

## Sample Sheet

A tab delimited sample sheet is required for the input samples to be processed. 

It must have the column names (in any order):

  * r1 - the filepath for the read 1 fastq in paired-end RNA-seq, or the single-end fastq file
    
  * r2 - the filepath for the read 2 fastq in paired-end RNA-seq
    
  * id - unique sample ID, no duplicates allowed in the sample sheet
    
  * single_end - boolean [true/false] if the data is single-end or paired-end 


**TO DO:** A function provided here, but it may not meet the needs of every experiment. 

```{r eval=TRUE}
example_sheet <- read.csv(here::here("test_data/paired_end_sample_sheet.csv"),
                            header = TRUE, 
                            comment.char = "#")
example_sheet
```

If downloading the fastq files directly from the SRA, the sample sheet only requires the `id` and the `single_end` columns. 

```{r eval=TRUE}
sra_example <- read.csv(here::here("test_data/sra_sample_sheet.csv"),
                        header = TRUE, 
                        comment.char = "#")

sra_example
```


## Nextflow Config

Edit the `nextflow.config` file to include the appropriate filepaths for the samples to be included in the pipeline, and the appropriate genome references. The required files are listed here:

```
workDir = "PATH/TO/SCRATCH" 

//global parameters
params {
    // general options
    sample_sheet                = "PATH/TO/SAMPLE_SHEET"
    queue                       = 'NAME OF QUEUE'
    project                     = 'PROJECT CODE'
    outdir                      = "PATH/TO/RESULTS"

    //star specific params. Must be full filepaths for files outside the projectDir
    index                       = 'PATH/TO/STAR_INDEX/ [optional if build_index == true]'
    build_index                 = false
    gtf                         = 'PATH/TO/GTF'
    fasta                       = 'PATH/TO/FASTA [optional if build_index == false ]'
    star_ignore_sjdbgtf         = false

    //trimgalore module specific parameters
    trim                        = [true/false]

    //RSEQC specific parameters
    gene_list                   = 'PATH/TO/RSEQC/rRNA.bed'
    ref_gene_model              = 'PATH/TO/RSEQC/gene_model.bed'
}
```

The `gene_list` and `ref_gene_model` can be found at the RSEQC [documentation page](http://rseqc.sourceforge.net/#download-gene-models-update-on-12-14-2021)
save these to the `/gpfs/shared_data` to share with the RSC team, if needed. It also is best practice to save the STAR indexes to `/gpfs/shared_data`. 


## Advanced Options

**TO DO:** add "Advanced Options" section to describe the `ext.args` under the process scope. 

# Run the workflow 

Then execute a wrapper around the `nextflow run main.nf` command which is in the `main_run.sh` shell script. Provide a descriptive name (string) for your workflow run, in this example we will use "my_analysis". 

The `main_run.sh` defines the profiles for difference executors in the variable `NFX_PROFILE`. The best choice is "PBS_singularity" which executes the jobs on the HPC using the PBS scheduler and then will run the job inside singularity containers with the appropriate software versions. 

```{bash}
./main_run.sh "my_analysis"
```

Typically, you will not need to change the `.main_run.sh` often. However, if needed for testing or development, the `NFX_PROFILE` can be changed to run locally, or with docker, or with conda. You can also change the entry_point for the workflow to run only the index building step using `NFX_ENTRY` set to `star_index`. 


# Expected Outputs 

Under the path provided in the nextflow config for params "outdir", you will find directories named for each of the modules. Lets say "params.outdir = ./results". There will be the following file structure:

results/

  * fastqc/ 
    * fastqc_{sample_id}_/
    
  * multiqc/
    * {sample_sheet_basename}_multiqc_report_data/ 
    * collects fastqc, star alignment, and star quantification stats 
    
  * rseqc/
    * {sample_id}.rRNA_stats.out
    * {sample_id}.in.bam - rRNA reads
    * {sample_id}.Aligned.sortedByCoord.out.summary.txt
    * {sample_id}.Aligned.sortedByCoord.out.tin.xls
    * {sample_id}.read_distribution.txt
    
  * samtools/
    * bam index (.bai) file
  
  * sratools/
    * {SRR_RUN_ID}.fastq.gz
    * sratoolkit config file
    
  * star/
    * {Sample_ID_001}.Aligned.out.bam
    * {Sample_ID_001}.Log.final.out
    * {Sample_ID_001}.Log.out
    * {Sample_ID_001}.Log.progress.out
    * {Sample_ID_001}.ReadsPerGene.out.tab
    * {Sample_ID_001}.SJ.out.tab

In addition, there will be an HTML report with information on where the temp data is stored in the `workDir` path, and general run statistics such as resource utilized  versus requested, which helps with optimization. It will also provide information on how much walltime was used per sample, total CPU hours, etc. 

The HTML file is found in `reports` directory and will have the prefix defined on the command line when the `./main_run.sh "my_analysis"` was invoked, so in this example it would be named "my_analysis_{DATE}.html". 

There will also be a detailed nextflow log file that is useful for de-bugging which will also be named in this example, "my_analysis_{DATE}_nextflow.log".

Finally, the pipeline will produce a DAG - Directed acyclic graph which describes the workflow channels (inputs) and the modules. The DAG image will be saved under `dag/` directory with the name "my_analysis_{DATE}_dag.pdf". 

# Share the Data

```{bash}
RESULTS="PATH/TO/PIPELINE/RESULTS/"
OUTDIR="path/to/collabs/RSS"
rsync -av $RESULTS $OUTDIR 
```


# Cleaning up Cached Data

Nextflow has an utility to [clean](https://www.nextflow.io/docs/latest/cli.html#clean) up old work directories and logs that are no longer needed. This can be run after x amount of time to keep your workdir from getting too large or if you're running out of disk space. 

This requires the session ID or session name, which can found in the `.nextflow/history` file. 

```{bash}
cat .nextflow/history
nextflow clean -f high_sinoussi
```



# Session Information

```{r}
sessionInfo()
```


